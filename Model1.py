# -*- coding: utf-8 -*-
"""fastTextPreProcess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uEg__ZzeHiFR7ZlkNgRp3iShFKDAr0gZ
"""

#FastText with Preprocessing
#No preprocessing performance > pre-processing performance (this system)

import zipfile
import pandas as pd
import re
import math
import time
from sklearn.model_selection import train_test_split
import numpy as np
import re
import nltk
import pickle
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')

#logisticregression Classifier
from sklearn import datasets
import pandas as pd
import numpy as np
import nltk
import re
import string

#upload dataset
from google.colab import files
uploaded = files.upload()

#check data is there
#upload boath okk.csv and ok.txt
!ls

#read CSV file and extract data

data_frame0 = pd.read_csv('okk.csv', header = None, encoding = 'utf-8')
trainText = data_frame0[0]
trainLabel = data_frame0[1]

stemmer = WordNetLemmatizer()
def pre_process(text):
    # Remove all the special characters
    text = re.sub(r'\W', ' ', str(text))
    # remove all single characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text)
    # Remove single characters from the start
    text = re.sub(r'\^[a-zA-Z]\s+', ' ', text) 
    # Substituting multiple spaces with single space
    text = re.sub(r'\s+', ' ', text, flags=re.I)
    # Removing prefixed 'b'
    text = re.sub(r'^b\s+', '', text)
    # Converting to Lowercase
    text = text.lower()
    # Lemmatization
    text = text.split()
    text = [stemmer.lemmatize(word) for word in text]
    text = ' '.join(text)
    
    return text

train = trainText.apply(pre_process)

#fast-text experimenting
!pip install fasttext
import fasttext

myModel = fasttext.train_supervised('ok.txt')

print(myModel.words)
print(myModel.labels)

myModel.predict("anon")

myModel.predict("I'm sad")

#following this tutorial: https://fasttext.cc/docs/en/supervised-tutorial.html
#def print_results(N, p, r):
    #print("N\t" + str(N))
    #print("P@{}\t{:.3f}".format(1, p))
    #print("R@{}\t{:.3f}".format(1, r))

#print_results(*myModel.test('ten.txt'))

#train contains the training data in pre-processed format
wordVectors = []
n = len(train)
i=0
#for i in trainText:
while i<n:
     wordVectors.append(myModel.get_sentence_vector(train[i]))
     i+=1

print(len(wordVectors))

print(len(trainLabel))

trainData = wordVectors

#SMOTE FOR OVERSAMPLING
from imblearn.over_sampling import SMOTE 
sm = SMOTE(random_state = 2) 
trainData, trainLabel = sm.fit_sample(trainData, trainLabel.ravel()) 
print("counts of label 'Harasser': {}".format(sum(trainLabel== 'Harasser'))) 
print("counts of label 'Victim': {} \n".format(sum(trainLabel == 'Victim')))

print("counts of label 'Harasser': {}".format(sum(trainLabel== 'Harasser'))) 
print("counts of label 'Victim': {} \n".format(sum(trainLabel == 'Victim')))

X_train, X_test, y_train, y_test = train_test_split(trainData, trainLabel, test_size=0.1,random_state=109)

from sklearn.linear_model import LogisticRegression

# fit model no training data
classifier = LogisticRegression()

#Train the model using the training sets
classifier.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = classifier.predict(X_test)

from sklearn import metrics

print("Identifying Harasser Metrics")
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred, pos_label = 'Harasser'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred, pos_label = 'Harasser'))

#measure F1 score
from sklearn.metrics import f1_score
print("F1 score: ", metrics.f1_score(y_test, y_pred, labels = None, pos_label = 'Harasser', average = 'binary', sample_weight = None))

print("")
#victim scores
print("Identifying Victim Metrics")


# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))


# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(y_test, y_pred, pos_label = 'Victim'))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(y_test, y_pred, pos_label = 'Victim'))

#measure F1 score
from sklearn.metrics import f1_score
print("F1 score: ", metrics.f1_score(y_test, y_pred, labels = None, pos_label = 'Victim', average = 'binary', sample_weight = None))


print("")
print("Weighted scores")
print("weighted F1:", metrics.f1_score(y_test, y_pred, average='weighted'))
print("weighted Precision:", metrics.precision_score(y_test, y_pred, average='weighted'))
print("weighted Recall:", metrics.recall_score(y_test, y_pred, average='weighted'))
# Model Error Rate
errorRate = 1-metrics.accuracy_score(y_test, y_pred)
print("Error Rate:", errorRate)





#got help from https://pypi.org/project/fasttext/